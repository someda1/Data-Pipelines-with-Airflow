# -*- coding: utf-8 -*-
"""Data Pipelines with Airflow-simeon omeda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11j9JuZZM6Z0TWCs5p4tdD09WC2U4yLQJ

# **Data Pipelines with Airflow-simeon omeda**

**Background Information**

Our telecommunications company, MTN Rwanda, has a vast customer base, and we generate a
large amount of data daily. We must efficiently process and store this data to make informed
business decisions. Therefore, we plan to develop a data pipeline to extract, transform, and load
data from three CSV files and store it in a Postgres database. We require a skilled data
engineer who can use the Airflow tool to develop the pipeline to achieve this.

**Problem Statement**

The main challenge is that the data generated is in a raw format, and we need to process it
efficiently to make it usable for analysis. This requires us to develop a data pipeline that can
extract, transform and load the data from multiple CSV files into a single database, which can
be used for further analysis.

**Guidelines**

The data pipeline should be developed using Airflow, an open-source tool for creating and
managing data pipelines. The following steps should be followed to develop the data pipeline:

● The data engineer should start by creating a DAG (Directed Acyclic Graph) that defines
the workflow of the data pipeline.

● The DAG should include tasks that extract data from the three CSV files.

● After extraction, the data should be transformed using Python libraries to match the
required format.

● Finally, the transformed data should be loaded into a Postgres database.

● The data pipeline should be scheduled to run at a specific time daily using the Airflow
scheduler.

● We can use the shared file (mtnrwanda-dag.py) as a starting point.
Sample CSV Files

The following are sample CSV files that will be used in the data pipeline:

● customer_data.csv

● order_data.csv

● payment_data.csv

All files for this project can be downloaded from here (link).
Deliverables
We will be expected to deliver a GitHub repository with the following:

● Airflow DAG file for the data pipeline.

● Documentation of the pipeline.

○ Highlight at least 3 best practices used during the implementation.

○ Recommendations for deployment and running the pipeline in a cloud-based
provider.
"""

!pip install apache-airflow

!pip install apache-airflow-providers-postgres

"""Prerequisites"""

#import libraries 
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
from airflow.operators.python import PythonOperator
import psycopg2
import csv
import json
from airflow.providers.postgres.hooks.postgres import PostgresHook

import logging
logging.basicConfig(filename='pipeline.log', level=logging.DEBUG)

#  creating a DAG (Directed Acyclic Graph) 
# DAG default arguments
default_args = {
    'owner': 'MTN Rwanda',
    'depends_on_past': False,
    'start_date': datetime(2023, 4, 18),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# DAG Definition
dag = DAG('data_pipeline', 
          default_args=default_args, 
          schedule_interval=timedelta(days=1)
          )

"""extract data from the three CSV files"""

# Data extraction from the CSV files
def extract_data():
    customer_df = pd.read_csv('customer_data.csv')
    order_df = pd.read_csv('order_data.csv')
    payment_df = pd.read_csv('payment_data.csv')
    return customer_df, order_df, payment_df

def transform_data(**kwargs): 
    
    """
    Function transforms data | merges dataframes | drops unneeded columns | returns a json string containing dictionary of values
    """
    
    json_transform=None
    
    try:
        jfc,jfo,jfp = kwargs['ti'].xcom_pull(task_ids='extract_data')
        
        json_dict_dfc = json.loads(jfc)
        json_dict_dfo = json.loads(jfo)
        json_dict_dfp = json.loads(jfp)
        
        dfc = pd.DataFrame.from_dict(json_dict_dfc)
        dfo = pd.DataFrame.from_dict(json_dict_dfo)
        dfp = pd.DataFrame.from_dict(json_dict_dfp)
        
        
        # convert date fields to the correct format using pd.to_datetime
        dfc['date_of_birth'] = pd.to_datetime(dfc['date_of_birth'])
        dfo['order_date'] = pd.to_datetime(dfo['order_date'])
        dfp['payment_date'] = pd.to_datetime(dfp['payment_date'])
        
        #add json serializable date str columns
        dfc['date_of_birth_str'] = dfc['date_of_birth'].apply(lambda x: x.strftime('%Y-%m-%d'))
        dfo['order_date_str'] = dfo['order_date'].apply(lambda x: x.strftime('%Y-%m-%d'))
        dfp['payment_date_str'] = dfp['payment_date'].apply(lambda x: x.strftime('%Y-%m-%d'))
        
        # merge customer and order dataframes on the customer_id column
        co_merge_df = pd.merge(dfc, dfo, on='customer_id')
        
        # merge payment dataframe with the merged dataframe on the order_id and customer_id columns
        pay_merge_df = pd.merge(co_merge_df, dfp, on=['customer_id', 'order_id'])
        
        # drop unnecessary columns like customer_id and order_id
        df = pay_merge_df.drop(['customer_id', 'order_id'], axis=1)
        
        # group the data by customer and aggregate the amount paid using sum
        grouped_df = df.groupby('first_name').agg({'amount': 'sum'})
        
        # create a new column to calculate the total value of orders made by each customer
        df['total_order_value'] = df.groupby('first_name')['amount'].transform('sum')
        
        # calculate the customer lifetime value using the formula CLV = (average order value) x (number of orders made per year) x (average customer lifespan) 
        # from visual inspection of merged_df - each customer has 1 order in 2023 - mean_order_value=sum_of_order, all orders made in 2023 ..hence lifespan =1yr
        df['clv'] = df['amount'] * 1 * 1
        
        #df with serializable timestamp columns 
        dfs = df.drop(['date_of_birth', 'order_date', 'payment_date'], axis=1)
        
        #convert df to dictionary
        dict_transform = dfs.to_dict()
        
        #serialize to json
        json_transform = json.dumps(dict_transform)
        
    except Exception as e:
        err = "Transform() error - "+ str(e)
        logging.debug(err)

    return json_transform

# Load the transformed data into a PostgreSQL database
def load_data(transformed_df):
    try:
        # Connect to the PostgreSQL database
        conn = psycopg2.connect(
            host = "34.170.193.146"
            database = "RwandaMTNDB"
            user = "admin"
            password = "admin1"
        )

        # Open a cursor to perform database operations
        cur = conn.cursor()

        # Create the customer_ltv table
        cur.execute("""
            CREATE TABLE IF NOT EXISTS customer_ltv (
                customer_id INTEGER PRIMARY KEY,
                total_orders INTEGER,
                total_amount NUMERIC(10,2),
                avg_order_value NUMERIC(10,2),
                ltv NUMERIC(10,2)
            )
        """)

        # Insert the transformed data into the customer_ltv table
        for index, row in transformed_df.iterrows():
            cur.execute("""
                INSERT INTO customer_ltv (customer_id, total_orders, total_amount, avg_order_value, ltv)
                VALUES (%s, %s, %s, %s, %s)
            """, (row['customer_id'], row['total_orders'], row['total_amount'], row['avg_order_value'], row['ltv']))

        conn.commit()

        # Close the cursor and connection
        cur.close()
        conn.close()

        # success message
        logging.info("Data loaded successfully into PostgreSQL database")

    except Exception as e:
        # error message
        logging.error(f"Error loading data into PostgreSQL database: {str(e)}")
        raise e

# extract data task
extract_data_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag
)

# transform data task
transform_data_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag
)

# load data task
load_data_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=dag
)

# task dependencies
extract_data_task >> transform_data_task >> load_data_task